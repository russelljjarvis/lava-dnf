{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87cd373e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<h1 align=\"center\">\n",
    "  Lava, Dynamic Neural Fields and the Loihi Software Ecosystem\n",
    "\n",
    "</h1>\n",
    "<h4 align=\"center\">\n",
    "    Russell Jarvis post-doctoral researcher at ICNS.\n",
    "</h4>\n",
    "\n",
    "\n",
    "<h2 align=\"center\">\n",
    "<img src=\"magma_versus_lava.png\" \n",
    "     width=\"300\" \n",
    "     height=\"350\" />\n",
    "</h2>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abbd705",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "<h2 align=\"center\">\n",
    "\n",
    "    Software Ecosystem and the relationship between Magma and Lava\n",
    "</h2>\n",
    "\n",
    "<h1 align=\"center\">\n",
    "<img src=\"loihi_diagram.png\" \n",
    "     width=\"500\" \n",
    "     height=\"650\" />\n",
    "</h1>\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7423d545",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<h2 align=\"center\">\n",
    "\n",
    "    Software Ecosystem: Lava is a composable coding paradigm.\n",
    "</h2>\n",
    "\n",
    "* Think Plug and Play\n",
    "* Lava architecture \"is inspired from the Communicating Sequential Process (CSP) concept for asynchronous, parallel systems that interact via message passing.\" -- Qoute from Lava README.md\n",
    "* \"A *Process* can thus be as simple as a single neuron or a synapse, as complex as a full neural network, and as non-neuromorphic as a streaming interface for a peripheral device or an executed instance of regular program code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5c1d2a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "### Since Lava is \"composable\", in theory older Loihi paradigms should be interopable with Lava:\n",
    "* Intel: NxSDK possibly the same thing as the Loihi Neurocore API\n",
    "* SNIPS a C code paradigm.\n",
    "\n",
    "### Direct Qoutes:\n",
    "\n",
    "* \"That the specific components of Magma needed to compile processes specific to Intel Loihi chips remains proprietary to Intel and is not provided through this GitHub site (see below). Similar Magma-layer code for other future commercial neuromorphic platforms likely will also remain proprietary.\" -- Qoute from Lava README.md\n",
    "* This is a relevant issue for ICNS, because we would like communicate with our board via an interface with Lava/Loihi respecting conventions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad19a58",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "     \n",
    "### Lava Dynamic Neural Fields Lava-DNF \n",
    "* DNF is the subset of the Lava paradigm that has a lot of useful patterns for specifying biological connectivity. \n",
    "* https://github.com/lava-nc/lava-dnf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c484dde1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Usefulness of Lava for large scale biological modelling work\n",
    "\n",
    "#### What we want from an interface\n",
    "\n",
    "- [x] Ability to define LIF Cell populations.\n",
    "- [x] Means to specify forwards connectivity between layers as populations.\n",
    "- [x] Means to specify recurrent connectivity whithin a population.\n",
    "- [x] Inhibitory Synapses (negative weight values possible)\n",
    "- [x] Capacity to support high cell counts.\n",
    "\n",
    "#### Not yet in the interface\n",
    "- [ ] Spike Timing Dependent Plasticity (STDP), or on chip local learning rules (coming).  \n",
    "- [ ] An ability to specify synaptic delays (hyper-synchronous epileptic network activity).  \n",
    "- [ ] Ability to visualize the layered architecture (nothing like TorchViz or TensorBoard for ANN architecture yet).\n",
    "- [ ] Delay Learning (possibly not a planned addition)\n",
    "- [ ] Adaptive Neurons (supported by SLAYER hard to make interoperable)\n",
    "- [ ] performance profiling (including power consumption). (coming)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d650465d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<h1 align=\"center\">\n",
    "  Below a small code demonstration to show how I established the tick box above a Lava-DNF.\n",
    "</h1>\n",
    "\n",
    "\n",
    "To evaluate whether Lava is useful for making bioplausible models, we want to define a variety of overlapping Spiking Neural Network (SNN) architectures, you can think of each one as a weighted directed graph: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236e6669",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "A graph is a function of Vertices and Edges:\n",
    "<h1 align=\"center\">\n",
    " $G(V, E)$\n",
    "</h1>\n",
    "\n",
    "Below is a diagram of the Potjan's cortical model. This model can be thought of as the composition of many weighted directed graphs, therefore we will use Lava a supported interface to begin to build a cortical model with the Python Loihi simulator.\n",
    "\n",
    "<h1 align=\"center\">\n",
    "\n",
    "<img src=\"Schematic-diagram-of-the-Potjans-Diesmann-cortical-microcircuit-model.png\" \n",
    "     width=\"300\" \n",
    "     height=\"350\" />\n",
    "\n",
    "</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb1d644",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Import relevant modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3971573e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "!export JUPYTER_PATH=\"${JUPYTER_PATH}:$(pwd)/src\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90982f25",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from lava.lib.dnf.operations.operations import Weights\n",
    "from lava.lib.dnf.operations.operations import *\n",
    "from lava.proc.lif.process import LIF\n",
    "from lava.lib.dnf.inputs.rate_code_spike_gen.process import RateCodeSpikeGen\n",
    "from lava.lib.dnf.connect.connect import connect\n",
    "from lava.lib.dnf.operations.operations import Weights\n",
    "from lava.magma.core.run_configs import Loihi1SimCfg #Loihi simulator, not  Loihi itself.\n",
    "from lava.magma.core.run_conditions import RunSteps\n",
    "from lava.proc.monitor.process import Monitor\n",
    "from lava.proc.monitor.models import PyMonitorModel\n",
    "from lava.lib.dnf.inputs.gauss_pattern.process import GaussPattern\n",
    "from lava.lib.dnf.kernels.kernels import MultiPeakKernel\n",
    "from lava.lib.dnf.utils.plotting import raster_plot\n",
    "from lava.lib.dnf.kernels.kernels import SelectiveKernel\n",
    "\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "\n",
    "from tutorial06_hierarchical_processes import *\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import elephant\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d71595",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pretend Cortical Model Specs:\n",
    "## These numbers are nominal. \n",
    "* The numbers simply fit the performance of a resource limited laptop\n",
    "* 2 columns.\n",
    "* 4 layers\n",
    "* 1 **E** pop and 1 **I** pop per layer.\n",
    "* 85 cells per population 170 cells per layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0559f719",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "### Create layerwise cell populations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe35b2de",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "ncolumns=2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1f47399",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Ex excitatory\n",
    "\n",
    "\n",
    "ly_2_3_ex = np.ndarray((ncolumns),dtype=object)\n",
    "ly_4_ex = np.ndarray((ncolumns),dtype=object)\n",
    "ly_5_ex = np.ndarray((ncolumns),dtype=object)\n",
    "ly_6_ex = np.ndarray((ncolumns),dtype=object)\n",
    "\n",
    "# In inhibitory\n",
    "ly_2_3_in = np.ndarray((ncolumns),dtype=object)\n",
    "ly_4_in = np.ndarray((ncolumns),dtype=object)\n",
    "ly_5_in = np.ndarray((ncolumns),dtype=object)\n",
    "ly_6_in = np.ndarray((ncolumns),dtype=object)\n",
    "\n",
    "\n",
    "for i in range(0,ncolumns):\n",
    "    ly_2_3_ex[i] = LIF(shape=(ncells,))\n",
    "    ly_4_ex[i] = LIF(shape=(ncells,))\n",
    "    ly_5_ex[i] = LIF(shape=(ncells,))\n",
    "    ly_6_ex[i] = LIF(shape=(ncells,))\n",
    "\n",
    "\n",
    "    ly_2_3_in[i] = LIF(shape=(ncells,))\n",
    "    ly_4_in[i] = LIF(shape=(ncells,))\n",
    "    ly_5_in[i] = LIF(shape=(ncells,))\n",
    "    ly_6_in[i] = LIF(shape=(ncells,))\n",
    "\n",
    "ncells = 85\n",
    "ncolumns = 2\n",
    "ly_2_3_ex = LIF(shape=(ncells, ncolumns))\n",
    "ly_4_ex = LIF(shape=(ncells, ncolumns))\n",
    "ly_5_ex = LIF(shape=(ncells, ncolumns))\n",
    "ly_6_ex = LIF(shape=(ncells, ncolumns))\n",
    "\n",
    "\n",
    "ly_2_3_in = LIF(shape=(ncells, ncolumns))\n",
    "ly_4_in = LIF(shape=(ncells, ncolumns))\n",
    "ly_5_in = LIF(shape=(ncells, ncolumns))\n",
    "ly_6_in = LIF(shape=(ncells, ncolumns))\n",
    "\n",
    "\n",
    "connections=[]\n",
    "exc_kernel = SelectiveKernel(amp_exc=0.15,\n",
    "                         width_exc=ncells/6.0,\n",
    "                         global_inh=0.0)\n",
    "\n",
    "#for i in range(0,ncolumns):\n",
    "    #ly_2_3_ex[i] 2 ly_2_3_ex[i]\n",
    "conv_kernel = connect(ly_2_3_ex.s_out, ly_2_3_ex.a_in, ops=[Weights(1.0)])\n",
    "connections.append(conv_kernel)\n",
    "\n",
    "#ly_4_ex[i] 2 ly_4_ex[i]            \n",
    "one2onec = connect(ly_4_ex.s_out, ly_4_ex.a_in, ops=[Weights(1.0)])\n",
    "connections.append(one2onec)\n",
    "\n",
    "#ly_5_ex[i] 2 ly_5_ex[i]\n",
    "one2onec = connect(ly_5_ex.s_out, ly_5_ex.a_in, ops=[Weights(1.0)])\n",
    "connections.append(one2onec)\n",
    "\n",
    "#ly_6_ex[i] 2 ly_6_ex[i]\n",
    "one2onec = connect(ly_6_ex.s_out, ly_6_ex.a_in, ops=[Weights(1.0)])\n",
    "connections.append(one2onec)\n",
    "\n",
    "conv_kernel = connect(ly_2_3_ex.s_out, ly_2_3_ex.a_in, ops=[Weights(1.0)])\n",
    "connections.append(conv_kernel)\n",
    "\n",
    "#ly_4_ex[i] 2 ly_4_ex[i]            \n",
    "one2onec = connect(ly_4_ex.s_out, ly_4_ex.a_in, ops=[Weights(1.0)])\n",
    "connections.append(one2onec)\n",
    "\n",
    "#ly_5_ex[i] 2 ly_5_ex[i]\n",
    "one2onec = connect(ly_5_ex.s_out, ly_5_ex.a_in, ops=[Weights(1.0)])\n",
    "connections.append(one2onec)\n",
    "\n",
    "#ly_6_ex[i] 2 ly_6_ex[i]\n",
    "one2onec = connect(ly_6_ex.s_out, ly_6_ex.a_in, ops=[Weights(1.0)])\n",
    "connections.append(one2onec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc978bb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab0f4dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n!pip install bmtk\\n!pip install sonata\\n\\n!git clone https://github.com/AllenInstitute/sonata\\n!git clone https://github.com/AllenInstitute/bmtk/\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "!pip install bmtk\n",
    "!pip install sonata\n",
    "\n",
    "!git clone https://github.com/AllenInstitute/sonata\n",
    "!git clone https://github.com/AllenInstitute/bmtk/\n",
    "'''    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "201f78aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n!cd bmtk/examples/point_450cells/network/\\n!ls -ltr bmtk/examples/point_450cells/network/\\nimport h5py    \\nimport numpy as np    \\nf1 = h5py.File(\"bmtk/examples/point_450cells/network/internal_internal_edges.h5\",\\'r+\\')  \\nf1[\\'edges\\']\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "!cd bmtk/examples/point_450cells/network/\n",
    "!ls -ltr bmtk/examples/point_450cells/network/\n",
    "import h5py    \n",
    "import numpy as np    \n",
    "f1 = h5py.File(\"bmtk/examples/point_450cells/network/internal_internal_edges.h5\",'r+')  \n",
    "f1['edges']\n",
    "'''\n",
    "#print(dir(f1))\n",
    "#f1\n",
    "\n",
    "#import pandas as pd\n",
    "#df = pd.read_hdf(\"bmtk/examples/point_450cells/network/internal_internal_edges.h5\")\n",
    "#df\n",
    "#df = pd.DataFrame(np.array(h5py.File(\"bmtk/examples/point_450cells/network/internal_internal_edges.h5\"))['edges'])\n",
    "#df\n",
    "#f1.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "400dacaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__del__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_find_attr_by_type', '_init_proc_member_obj', '_is_compiled', '_model', '_post_init', '_runtime', 'a_in', 'bias', 'bias_exp', 'compile', 'du', 'dv', 'id', 'in_ports', 'init_args', 'is_compiled', 'is_sub_proc_of', 'load', 'name', 'out_ports', 'parent_proc', 'pause', 'proc_params', 'procs', 'ref_ports', 'register_sub_procs', 'run', 'runtime', 's_out', 'save', 'shape', 'stop', 'u', 'v', 'validate_var_aliases', 'var_ports', 'vars', 'vth', 'wait']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'LIF' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m exc_kernel \u001b[38;5;241m=\u001b[39m SelectiveKernel(amp_exc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.15\u001b[39m,\n\u001b[1;32m      5\u001b[0m                          width_exc\u001b[38;5;241m=\u001b[39mncells\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m6.0\u001b[39m,\n\u001b[1;32m      6\u001b[0m                          global_inh\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,ncolumns):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m#ly_2_3_ex[i] 2 ly_2_3_ex[i]\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     conv_kernel \u001b[38;5;241m=\u001b[39m connect(\u001b[43mly_2_3_ex\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39ms_out, ly_2_3_ex[i]\u001b[38;5;241m.\u001b[39ma_in, ops\u001b[38;5;241m=\u001b[39m[Weights(\u001b[38;5;241m1.0\u001b[39m)])\n\u001b[1;32m     11\u001b[0m     connections\u001b[38;5;241m.\u001b[39mappend(conv_kernel)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m#ly_4_ex[i] 2 ly_4_ex[i]            \u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'LIF' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "print(dir(ly_2_3_ex))\n",
    "\n",
    "connections=[]\n",
    "exc_kernel = SelectiveKernel(amp_exc=0.15,\n",
    "                         width_exc=ncells/6.0,\n",
    "                         global_inh=0.0)\n",
    "\n",
    "for i in range(0,ncolumns):\n",
    "    #ly_2_3_ex[i] 2 ly_2_3_ex[i]\n",
    "    conv_kernel = connect(ly_2_3_ex[i].s_out, ly_2_3_ex[i].a_in, ops=[Weights(1.0)])\n",
    "    connections.append(conv_kernel)\n",
    "    \n",
    "    #ly_4_ex[i] 2 ly_4_ex[i]            \n",
    "    one2onec = connect(ly_4_ex[i].s_out, ly_4_ex[i].a_in, ops=[Weights(1.0)])\n",
    "    connections.append(one2onec)\n",
    "\n",
    "    #ly_5_ex[i] 2 ly_5_ex[i]\n",
    "    one2onec = connect(ly_5_ex[i].s_out, ly_5_ex[i].a_in, ops=[Weights(1.0)])\n",
    "    connections.append(one2onec)\n",
    "\n",
    "    #ly_6_ex[i] 2 ly_6_ex[i]\n",
    "    one2onec = connect(ly_6_ex[i].s_out, ly_6_ex[i].a_in, ops=[Weights(1.0)])\n",
    "    connections.append(one2onec)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f7e02e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h1 align=\"center\"> Create the connectivity pattern </h1>\n",
    "\n",
    "<h3> \n",
    "There are three obvious ways to define connectivity:\n",
    "    * Kernels\n",
    "    * One to One\n",
    "    * Connectivity Matrix.    \n",
    "</h3>\n",
    "\n",
    "<h4> \n",
    "* First I apply the repeated/stereotyped connections whithin and between layers:\n",
    "</h4>\n",
    "\n",
    "\n",
    "\n",
    "<!---\n",
    "\n",
    "<h1 align=\"center\">\n",
    "    <img src=\"Schematic-diagram-of-the-Potjans-Diesmann-cortical-microcircuit-model.png\" \n",
    "         width=\"300\" \n",
    "         height=\"350\" />\n",
    "</h1>\n",
    "\n",
    "* from the Potjan's wiring diagram\n",
    "researchgate.net/figure/Schematic-diagram-of-the-Potjans-Diesmann-cortical-microcircuit-model_fig1_349443713\n",
    "\n",
    "--->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60d52b9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "connections=[]\n",
    "exc_kernel = SelectiveKernel(amp_exc=0.15,\n",
    "                         width_exc=ncells/6.0,\n",
    "                         global_inh=0.0)\n",
    "\n",
    "for i in range(0,ncolumns):\n",
    "    #ly_2_3_ex[i] 2 ly_2_3_ex[i]\n",
    "    conv_kernel = connect(ly_2_3_ex[i].s_out, ly_2_3_ex[i].a_in, ops=[Weights(1.0)])\n",
    "    connections.append(conv_kernel)\n",
    "    \n",
    "    #ly_4_ex[i] 2 ly_4_ex[i]            \n",
    "    one2onec = connect(ly_4_ex[i].s_out, ly_4_ex[i].a_in, ops=[Weights(1.0)])\n",
    "    connections.append(one2onec)\n",
    "\n",
    "    #ly_5_ex[i] 2 ly_5_ex[i]\n",
    "    one2onec = connect(ly_5_ex[i].s_out, ly_5_ex[i].a_in, ops=[Weights(1.0)])\n",
    "    connections.append(one2onec)\n",
    "\n",
    "    #ly_6_ex[i] 2 ly_6_ex[i]\n",
    "    one2onec = connect(ly_6_ex[i].s_out, ly_6_ex[i].a_in, ops=[Weights(1.0)])\n",
    "    connections.append(one2onec)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da59148f",
   "metadata": {},
   "source": [
    "## One to one synapse projections between layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae6020b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_conn_sankey(conv_kernel,ncells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9ca862",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in range(0,ncolumns):\n",
    "    #ly_2_3_in[i] 2 ly_2_3_in[i]\n",
    "    one2onec = connect(ly_2_3_in[i].s_out, ly_2_3_in[i].a_in, ops=[Weights(-1.0)])\n",
    "    connections.append(one2onec)\n",
    "    #ly_4_in[i] 2 ly_4_in[i]\n",
    "    one2onec = connect(ly_4_in[i].s_out, ly_4_in[i].a_in, ops=[Weights(-1.0)])\n",
    "    connections.append(one2onec)\n",
    "    #ly_5_in[i] 2 ly_5_in[i]\n",
    "    one2onec = connect(ly_5_in[i].s_out, ly_5_in[i].a_in, ops=[Weights(-1.0)])\n",
    "    connections.append(one2onec)\n",
    "    #ly_6_in[i] 2 ly_6_in[i]\n",
    "    one2onec = connect(ly_6_in[i].s_out, ly_6_in[i].a_in, ops=[Weights(-1.0)])\n",
    "    connections.append(one2onec)\n",
    "\n",
    "for i in range(0,ncolumns):\n",
    "    #ly_2_3_ex[i] 2 ly_2_3_in[i]\n",
    "    one2onec = connect(ly_2_3_ex[i].s_out, ly_2_3_in[i].a_in, ops=[Weights(1.0)])\n",
    "    connections.append(one2onec)\n",
    "    #ly_4_ex[i] 2 ly_4_in[i]\n",
    "    one2onec = connect(ly_4_ex[i].s_out, ly_4_in[i].a_in, ops=[Weights(1.0)])\n",
    "    connections.append(one2onec)\n",
    "    #ly_5_ex[i] 2 ly_5_in[i]\n",
    "    one2onec = connect(ly_5_ex[i].s_out, ly_5_in[i].a_in, ops=[Weights(1.0)])\n",
    "    connections.append(one2onec)\n",
    "    #ly_6_ex[i] 2 ly_6_in[i]\n",
    "    one2onec = connect(ly_6_ex[i].s_out, ly_6_in[i].a_in, ops=[Weights(1.0)])\n",
    "    connections.append(one2onec)\n",
    "\n",
    "for i in range(0,ncolumns):\n",
    "    #ly_2_3_in[i] 2 ly_2_3_exc[i]\n",
    "    one2onec = connect(ly_2_3_in[i].s_out, ly_2_3_ex[i].a_in, ops=[Weights(-1.0)])\n",
    "    connections.append(one2onec)\n",
    "    #ly_4_in[i] 2 ly_4_exc[i]\n",
    "    one2onec = connect(ly_4_in[i].s_out, ly_4_ex[i].a_in, ops=[Weights(-1.0)])\n",
    "    connections.append(one2onec)\n",
    "    #ly_5_in[i] 2 ly_5_exc[i]\n",
    "    one2onec = connect(ly_5_in[i].s_out, ly_5_ex[i].a_in, ops=[Weights(-1.0)])\n",
    "    connections.append(one2onec)\n",
    "    #ly_6_in[i] 2 ly_6_exc[i]\n",
    "    one2onec = connect(ly_6_in[i].s_out, ly_6_ex[i].a_in, ops=[Weights(-1.0)])\n",
    "    connections.append(one2onec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edd52f3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<!--\n",
    "\n",
    "### Wanted Convergence (green and red) and Divergence (blue)\n",
    "\n",
    "<h1 align=\"center\">\n",
    "    <img src=\"conv_div.png\" \n",
    "         width=\"280\" \n",
    "         height=\"280\"/>\n",
    "</h1>\n",
    "\n",
    "## Lava DNF has some interesting divergence patterns\n",
    "\n",
    "```python\n",
    "exc_kernel = SelectiveKernel(amp_exc=0.15,\n",
    "                         width_exc=ncells/6.0,\n",
    "                         global_inh=0.0)\n",
    "```                         \n",
    "\n",
    "\n",
    "* Up to now the network has no convergence and limited divergence patterns between cells.\n",
    ", so its not so much a network, and more of a series of parallel feedforward lines, like in the figure below:\n",
    "\n",
    "<h1 align=\"center\">\n",
    "    <img src=\"one_to_one_connectivity.png\" \n",
    "         width=\"45\" \n",
    "         height=\"15\"/>\n",
    "</h1>\n",
    "\n",
    "### Divergence and some very specific connections between layer connections:\n",
    "-->\n",
    "## Smeared Regular Divergence\n",
    "### Some to some connectivity (not one to one, or one to many)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f5bea3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(0,ncolumns):\n",
    "    one2onec = connect(ly_2_3_ex[i].s_out, ly_5_ex[i].s_out, ops=[Weights(1.0)])\n",
    "    connections.append(one2onec)\n",
    "\n",
    "    one2onec = connect(ly_2_3_ex[i].s_out, ly_4_in[i].a_in, ops=[Convolution(exc_kernel)])\n",
    "    connections.append(one2onec)\n",
    "\n",
    "    one2onec = connect(ly_2_3_ex[i].s_out, ly_5_in[i].a_in, ops=[Convolution(exc_kernel)])\n",
    "    connections.append(one2onec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d448235",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig = px.imshow(connections[-1].weights.init)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df782b1",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(0,ncolumns):\n",
    "    one2onec = connect(ly_5_ex[i].s_out, ly_4_in[i].a_in, ops=[Weights(1.0)])\n",
    "    connections.append(one2onec)\n",
    "    one2onec = connect(ly_6_ex[i].s_out, ly_4_ex[i].a_in, ops=[Convolution(exc_kernel)])\n",
    "    connections.append(one2onec)\n",
    "    one2onec = connect(ly_6_ex[i].s_out, ly_2_3_in[i].a_in, ops=[Convolution(exc_kernel)])\n",
    "    connections.append(one2onec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28534bf1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Kernel for divergent inhibition\n",
    "```python\n",
    "inh_kernel = MultiPeakKernel(amp_exc=0.0,\n",
    "                         width_exc=1.0,\n",
    "                         amp_inh=-0.1,\n",
    "                         width_inh=ncells/10)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4059a52",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "inh_kernel = MultiPeakKernel(amp_exc=0.0,\n",
    "                         width_exc=1.0,\n",
    "                         amp_inh=-0.1,\n",
    "                         width_inh=ncells/10)\n",
    "\n",
    "for i in range(0,ncolumns):\n",
    "    one2onec = connect(ly_2_3_ex[i].s_out, ly_2_3_in[i].a_in, ops=[Convolution(inh_kernel)])\n",
    "    connections.append(one2onec)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbcbb51",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_conn_sankey(connections[-1],ncells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c35ef5",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(0,ncolumns):\n",
    "    one2onec = connect(ly_4_ex[i].s_out, ly_2_3_ex[i].s_out, ops=[Weights(1.0)])\n",
    "    connections.append(one2onec)\n",
    "    one2onec = connect(ly_4_ex[i].s_out, ly_5_ex[i].a_in, ops=[Weights(1.0)])\n",
    "    connections.append(one2onec)\n",
    "    one2onec = connect(ly_5_ex[i].s_out, ly_6_ex[i].a_in, ops=[Weights(1.0)])\n",
    "    connections.append(one2onec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b44243",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* **Specify** a direct connection between each of the two columns as a demonstration only\n",
    "layer 2-3 of Column2 (index 1) gets no tonic input, its input is just the output of column 1 (index 0)\n",
    "ly_2_3_ex[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb801e48",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "one2onec = connect(ly_2_3_ex[0].s_out, ly_2_3_ex[1].s_out, ops=[Weights(1.0)])\n",
    "connections.append(one2onec)\n",
    "\n",
    "dim=(ncells,ncells)\n",
    "weights0 = 0.0125*np.random.rand(ncells,ncells)\n",
    "weights1 = weights0\n",
    "#instantiate 2 DenseLayers\n",
    "layer0 = DenseLayer(shape=dim,weights=weights0, bias=4, vth=10)\n",
    "layer1 = DenseLayer(shape=dim,weights=weights1, bias=4, vth=10)\n",
    "#connect layer 0 to layer 1\n",
    "layer0.s_out.connect(layer1.s_in)\n",
    "\n",
    "many2onec = connect(ly_2_3_in[i].s_out, layer0.s_in, ops=[Weights(0.025)])\n",
    "one2onec = connect(ly_2_3_ex[0].s_out, ly_2_3_ex[1].s_out, ops=[Weights(1.0)])\n",
    "connections.append(one2onec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036b5794",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dim=(ncells,ncells)\n",
    "weights0 = 0.0125*np.ones((ncells,ncells))\n",
    "weights1 = weights0\n",
    "print(weights0)\n",
    "#instantiate 2 DenseLayers\n",
    "layer0 = DenseLayer(shape=dim,weights=weights0, bias=4, vth=10)\n",
    "layer1 = DenseLayer(shape=dim,weights=weights1, bias=4, vth=10)\n",
    "#connect layer 0 to layer 1\n",
    "\n",
    "layer0.s_out.connect(layer1.s_in)\n",
    "many2onec = connect(ly_2_3_in[i].s_out, layer0.s_in, ops=[Weights(-0.15)])\n",
    "connections.append(many2onec)\n",
    "\n",
    "many2onec = connect(ly_2_3_ex[i].s_out, layer0.s_in, ops=[Weights(0.05)])\n",
    "#print('Layer 1 weights: \\n', layer1.weights.get(),'\\n')\n",
    "connections.append(many2onec)\n",
    "\n",
    "many2onec = connect(ly_2_3_ex[i].s_out, layer0.s_in, ops=[Weights(0.05)])\n",
    "connections.append(one2onec)\n",
    "#print('Layer 1 weights: \\n', layer1.weights.get(),'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b83952",
   "metadata": {},
   "source": [
    "# Divergence and Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf9f75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One to many\n",
    "many2onec = connect(ly_2_3_in[0].out_ports.members[0] ,layer0.s_in, ops=[Weights(0.05)])\n",
    "connections.append(one2onec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc5b1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Many to convergence.\n",
    "many2onec = connect(ly_2_3_in[0].out_ports.members[0] ,layer0.in_ports.members[0], ops=[Weights(0.05)])\n",
    "connections.append(one2onec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842b4ea7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "# TODO:\n",
    "\n",
    "- [ ] Use the OSB PyNN model to wire layer to layer connections with established probabilities from Potjan's.\n",
    "https://github.com/NeuralEnsemble/PyNN/blob/master/examples/Potjans2014/network.py\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf0a62a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Create tonic input for the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d44982",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "spike_generator_1 = RateCodeSpikeGen(shape=ncells,)\n",
    "center_cell=ncells/2.0\n",
    "spread_across_number_of_cells=ncells \n",
    "gauss_pattern_1 = GaussPattern(shape=ncells,\n",
    "                               amplitude=80,\n",
    "                               mean=center_cell,\n",
    "                               stddev=spread_across_number_of_cells)\n",
    "gauss_pattern_1.a_out.connect(spike_generator_1.a_in)\n",
    "\n",
    "for i in range(0,ncolumns):\n",
    "    _=connect(spike_generator_1.s_out, ly_4_ex[i].a_in, [Weights(1.0)])\n",
    "    _=connect(spike_generator_1.s_out, ly_4_in[i].a_in, [Weights(0.125)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21c2fef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Run the preliminary Potjans model on CPU for 250ms\n",
    "\n",
    "One awesome property of the Lava paradigm you only have to run a segment of the model to run the whole model.\n",
    "\n",
    "Model segments seem to have parent child relationships with other segments if they are connected with network connections, and the Loihi compiler seems to understand that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585be2a2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Set up the experimental recording rig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f6fae9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "time_steps = 250\n",
    "monitor_layer0 = Monitor()\n",
    "monitor_layer0.probe(target=layer0.s_out, num_steps=time_steps)\n",
    "monitor_ly_2_3_ex = Monitor()\n",
    "monitor_ly_2_3_ex.probe(target=ly_2_3_ex[0].s_out, num_steps=time_steps)\n",
    "monitor_ly_2_3_in = Monitor()\n",
    "monitor_ly_2_3_in.probe(target=ly_2_3_in[0].s_out, num_steps=time_steps)\n",
    "monitor_ly_4_ex = Monitor()\n",
    "monitor_ly_4_ex.probe(target=ly_4_ex[0].s_out, num_steps=time_steps)\n",
    "monitor_input_1 = Monitor()\n",
    "monitor_input_1.probe(spike_generator_1.s_out, time_steps)\n",
    "other_column = Monitor()\n",
    "other_column.probe(ly_2_3_ex[1].s_out, time_steps)\n",
    "ly_4_ex[0].amplitude = 100\n",
    "ly_4_ex[0].run(condition=RunSteps(num_steps=time_steps),\n",
    "        run_cfg=Loihi1SimCfg(select_tag='floating_pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed31955",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Get probed data from monitors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f066af5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "data_input1 = monitor_input_1.get_data()\\\n",
    "    [spike_generator_1.name][spike_generator_1.s_out.name]\n",
    "data_ly_2_3_ex = monitor_ly_2_3_ex.get_data()\\\n",
    "    [ly_2_3_ex[0].name][ly_2_3_ex[0].s_out.name]\n",
    "data_ly_2_3_in = monitor_ly_2_3_in.get_data()\\\n",
    "    [ly_2_3_in[0].name][ly_2_3_in[0].s_out.name]\n",
    "data_ly_4_ex = monitor_ly_4_ex.get_data()\\\n",
    "    [ly_4_ex[0].name][ly_4_ex[0].s_out.name]\n",
    "data_ly_4_ex = monitor_ly_4_ex.get_data()\\\n",
    "    [ly_4_ex[0].name][ly_4_ex[0].s_out.name]\n",
    "\n",
    "data_l1=monitor_layer0.get_data()\\\n",
    "[layer0.name][layer0.s_out.name]\n",
    "\n",
    "#print(data_l1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061290bb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Plot the Spiking Data\n",
    "### Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6b1612",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "raster_plot(data_input1.T)\n",
    "cv = compute_cv(data_input1)\n",
    "print(\"The coefficient of variation is {0}\".format(cv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8dbe82",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "raster_plot(data_l1.T)\n",
    "cv = compute_cv(data_l1.T)\n",
    "\n",
    "print(\"The coefficient of variation is {0}\".format(cv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c34826e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "raster_plot(data_ly_2_3_ex.T)\n",
    "cv = compute_cv(data_ly_2_3_ex)\n",
    "print(\"The coefficient of variation is {0}\".format(cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9c5850",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "raster_plot(data_ly_4_ex.T)\n",
    "cv = compute_cv(data_ly_4_ex)\n",
    "print(\"The coefficient of variation is {0}\".format(cv))\n",
    "total = np.hstack([data_ly_2_3_ex.T,data_ly_4_ex.T])\n",
    "raster_plot(total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e1b4c7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "raster_plot(data_ly_2_3_in.T)\n",
    "cv = compute_cv(data_ly_2_3_in)\n",
    "print(\"The coefficient of variation is {0}\".format(cv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c096d7b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "raster_plot(data_ly_2_3_ex.T)\n",
    "cv = compute_cv(data_ly_2_3_ex)\n",
    "print(\"The coefficient of variation is {0}\".format(cv))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea71e600",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Check to see spiking activity propogated from one column to the other via a dedicated connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68b00dd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "data_ly_2_3_ex_other = other_column.get_data()\\\n",
    "    [ly_2_3_ex[1].name][ly_2_3_ex[1].s_out.name]\n",
    "raster_plot(data_ly_2_3_ex_other.T)\n",
    "cv = compute_cv(data_ly_2_3_ex_other)\n",
    "print(\"The coefficient of variation is {0}\".format(cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d954015b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### My verdict the overall approach works for some things but not others.\n",
    "\n",
    "\n",
    "\n",
    "- [x] Fan out achievable with connection kernels.\n",
    "- [x] Mexican hat weight distributions (lateral inhibition) can be coded.\n",
    "- [x] Means to specify forwards connectivity between populations.\n",
    "- [x] Means to specify recurrent connectivity between populations.\n",
    "- [x] Ability to define LIF Cell populations.\n",
    "- [x] Inhibitory Synapses (negative weight values possible)\n",
    "- [x] Capacity to support high cell counts.\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bd107a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Missing\n",
    "\n",
    "\n",
    "- [ ] Although Lava is advertised as composable (interopable with external code) It is unclear how: \n",
    "* arbitrary model of STDP can be translated through the Loihi compiler.\n",
    "* It's not clear how composable the LAVA approach is with NxSDK and SNIPs \n",
    "- [ ] (without breaking Loihi compilation).\n",
    "- [ ] Loihi compilation is still intentionally opaque (proprietary code).\n",
    "- [ ] Ability to visualize the whole architecture (nothing like TorchViz for ANN architecture yet).\n",
    "- [ ] Delay Learning \n",
    "- [ ] performance profiling (including power consumption). (coming)\n",
    "- [ ] No synaptic learning capability yet. \n",
    "\n",
    "## Concerns\n",
    "* Convencience methods for assigning delay distributions to weights.\n",
    "* Lack of examples of the possibility to to hack in old stuff that already works on Loihi like STDP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652a1a12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
